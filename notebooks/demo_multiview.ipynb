{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pl\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.spatial.transform import Rotation\n",
    "import plotly.graph_objects as go\n",
    "import plotly.subplots as sp\n",
    "import trimesh\n",
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import rootutils\n",
    "rootutils.setup_root(\"/opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/jianingy/research/accel-cortex/dust3r/fast3r/src\", indicator=\".project-root\", pythonpath=True)\n",
    "\n",
    "from src.dust3r.inference_multiview import inference\n",
    "from src.dust3r.model import FlashDUSt3R\n",
    "from src.dust3r.utils.image import load_images, rgb\n",
    "from src.dust3r.viz import CAM_COLORS, OPENGL, add_scene_cam, cat_meshes, pts3d_to_trimesh\n",
    "\n",
    "\n",
    "pl.ion()\n",
    "\n",
    "\n",
    "def get_reconstructed_scene(\n",
    "    outdir,\n",
    "    model,\n",
    "    device,\n",
    "    silent,\n",
    "    image_size,\n",
    "    filelist,\n",
    "    dtype=torch.float32,\n",
    "):\n",
    "    \"\"\"\n",
    "    from a list of images, run dust3r inference, global aligner.\n",
    "    then run get_3D_model_from_scene\n",
    "    \"\"\"\n",
    "    multiple_views_in_one_sample = load_images(filelist, size=image_size, verbose=not silent)\n",
    "\n",
    "    # time the inference\n",
    "    start = time.time()\n",
    "    output = inference(multiple_views_in_one_sample, model, device, dtype=dtype, verbose=not silent)\n",
    "    end = time.time()\n",
    "    print(f\"Time elapsed: {end - start}\")\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def plot_rgb_images(views, title=\"RGB Images\"):\n",
    "    fig = sp.make_subplots(rows=1, cols=len(views), subplot_titles=[f\"View {i} Image\" for i in range(len(views))])\n",
    "\n",
    "    # Plot the RGB images\n",
    "    for i, view in enumerate(views):\n",
    "        img_rgb = view['img'].cpu().numpy().squeeze().transpose(1, 2, 0)  # Shape: (224, 224, 3)\n",
    "        # Rescale RGB values from [-1, 1] to [0, 255]\n",
    "        img_rgb = ((img_rgb + 1) * 127.5).astype(int).clip(0, 255)\n",
    "        \n",
    "        fig.add_trace(go.Image(z=img_rgb), row=1, col=i+1)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        margin=dict(l=0, r=0, b=0, t=40)\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "def plot_confidence_maps(preds, title=\"Confidence Maps\"):\n",
    "    fig = sp.make_subplots(rows=1, cols=len(preds), subplot_titles=[f\"View {i} Confidence\" for i in range(len(preds))])\n",
    "\n",
    "    # Plot the confidence maps\n",
    "    for i, pred in enumerate(preds):\n",
    "        conf = pred['conf'].cpu().numpy().squeeze()\n",
    "        fig.add_trace(go.Heatmap(z=conf, colorscale='Viridis', showscale=False), row=1, col=i+1)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        margin=dict(l=0, r=0, b=0, t=40)\n",
    "    )\n",
    "\n",
    "    for i in range(len(preds)):\n",
    "        fig['layout'][f'yaxis{i+1}'].update(autorange='reversed')\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "def plot_3d_points_with_colors(preds, views, title=\"3D Points Visualization\", flip_axes=False, as_mesh=False, min_conf_thr_percentile=80, export_ply_path=None):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    all_points = []\n",
    "    all_colors = []\n",
    "    \n",
    "    if as_mesh:\n",
    "        meshes = []\n",
    "        for i, pred in enumerate(preds):\n",
    "            pts3d = pred['pts3d_in_other_view'].cpu().numpy().squeeze()  # Ensure tensor is on CPU and convert to numpy\n",
    "            img_rgb = views[i]['img'].cpu().numpy().squeeze().transpose(1, 2, 0)  # Shape: (224, 224, 3)\n",
    "            conf = pred['conf'].cpu().numpy().squeeze()\n",
    "\n",
    "            # Determine the confidence threshold based on the percentile\n",
    "            conf_thr = np.percentile(conf, min_conf_thr_percentile)\n",
    "\n",
    "            # Filter points based on the confidence threshold\n",
    "            mask = conf > conf_thr\n",
    "\n",
    "            # Rescale RGB values from [-1, 1] to [0, 255]\n",
    "            img_rgb = ((img_rgb + 1) * 127.5).astype(np.uint8).clip(0, 255)\n",
    "\n",
    "            # Generate the mesh for the current view\n",
    "            mesh_dict = pts3d_to_trimesh(img_rgb, pts3d, valid=mask)\n",
    "            meshes.append(mesh_dict)\n",
    "\n",
    "        # Concatenate all meshes\n",
    "        combined_mesh = trimesh.Trimesh(**cat_meshes(meshes))\n",
    "\n",
    "        # Flip axes if needed\n",
    "        if flip_axes:\n",
    "            combined_mesh.vertices[:, [1, 2]] = combined_mesh.vertices[:, [2, 1]]\n",
    "            combined_mesh.vertices[:, 2] = -combined_mesh.vertices[:, 2]\n",
    "\n",
    "        # Export as .ply if the path is provided\n",
    "        if export_ply_path:\n",
    "            combined_mesh.export(export_ply_path)\n",
    "\n",
    "        # Add the combined mesh to the plotly figure\n",
    "        vertex_colors = combined_mesh.visual.vertex_colors[:, :3]  # Ensure the colors are in RGB format\n",
    "        # Map vertex colors to face colors\n",
    "        face_colors = []\n",
    "        for face in combined_mesh.faces:\n",
    "            face_colors.append(np.mean(vertex_colors[face], axis=0))\n",
    "        face_colors = np.array(face_colors).astype(int)\n",
    "        face_colors = ['rgb({}, {}, {})'.format(r, g, b) for r, g, b in face_colors]\n",
    "\n",
    "        fig.add_trace(go.Mesh3d(\n",
    "            x=combined_mesh.vertices[:, 0], \n",
    "            y=combined_mesh.vertices[:, 1], \n",
    "            z=combined_mesh.vertices[:, 2],\n",
    "            i=combined_mesh.faces[:, 0], \n",
    "            j=combined_mesh.faces[:, 1], \n",
    "            k=combined_mesh.faces[:, 2],\n",
    "            facecolor=face_colors,\n",
    "            opacity=0.5,\n",
    "            name=\"Combined Mesh\"\n",
    "        ))\n",
    "    else:\n",
    "        # Loop through each set of points in preds\n",
    "        for i, pred in enumerate(preds):\n",
    "            pts3d = pred['pts3d_in_other_view'].cpu().numpy().squeeze()  # Ensure tensor is on CPU and convert to numpy\n",
    "            img_rgb = views[i]['img'].cpu().numpy().squeeze().transpose(1, 2, 0)  # Shape: (224, 224, 3)\n",
    "            conf = pred['conf'].cpu().numpy().squeeze()\n",
    "\n",
    "            # Determine the confidence threshold based on the percentile\n",
    "            conf_thr = np.percentile(conf, min_conf_thr_percentile)\n",
    "\n",
    "            # Flatten the points and colors\n",
    "            x, y, z = pts3d[..., 0].flatten(), pts3d[..., 1].flatten(), pts3d[..., 2].flatten()\n",
    "            r, g, b = img_rgb[..., 0].flatten(), img_rgb[..., 1].flatten(), img_rgb[..., 2].flatten()\n",
    "            conf_flat = conf.flatten()\n",
    "\n",
    "            # Apply confidence mask\n",
    "            mask = conf_flat > conf_thr\n",
    "            x, y, z = x[mask], y[mask], z[mask]\n",
    "            r, g, b = r[mask], g[mask], b[mask]\n",
    "\n",
    "            # Collect points and colors for exporting\n",
    "            all_points.append(np.vstack([x, y, z]).T)\n",
    "            all_colors.append(np.vstack([r, g, b]).T)\n",
    "\n",
    "            # Rescale RGB values from [-1, 1] to [0, 255]\n",
    "            r = ((r + 1) * 127.5).astype(int).clip(0, 255)\n",
    "            g = ((g + 1) * 127.5).astype(int).clip(0, 255)\n",
    "            b = ((b + 1) * 127.5).astype(int).clip(0, 255)\n",
    "\n",
    "            colors = ['rgb({}, {}, {})'.format(r[j], g[j], b[j]) for j in range(len(r))]\n",
    "            \n",
    "            # Check the flag and flip axes if needed\n",
    "            if flip_axes:\n",
    "                x, y, z = x, z, y\n",
    "                z = -z\n",
    "\n",
    "            # Add points to the plot\n",
    "            fig.add_trace(go.Scatter3d(\n",
    "                x=x, y=y, z=z,\n",
    "                mode='markers',\n",
    "                marker=dict(size=2, opacity=0.8, color=colors),\n",
    "                name=f\"View {i}\"\n",
    "            ))\n",
    "\n",
    "        # Export as .ply if the path is provided\n",
    "        if export_ply_path:\n",
    "            all_points = np.vstack(all_points)\n",
    "            all_colors = np.vstack(all_colors)\n",
    "            point_cloud = trimesh.PointCloud(vertices=all_points, colors=all_colors)\n",
    "            point_cloud.export(export_ply_path)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        scene=dict(\n",
    "            xaxis_title='X',\n",
    "            yaxis_title='Y',\n",
    "            zaxis_title='Z'\n",
    "        ),\n",
    "        margin=dict(l=0, r=0, b=0, t=40),\n",
    "        height=1000\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import plotly.graph_objects as go\n",
    "from src.dust3r.cloud_opt.init_im_poses import fast_pnp\n",
    "from src.dust3r.viz import auto_cam_size\n",
    "from src.dust3r.viz_plotly import SceneViz\n",
    "from src.dust3r.utils.image import rgb  # Assuming you have this utility for image processing\n",
    "\n",
    "# Function to estimate camera poses using fast_pnp\n",
    "def estimate_camera_poses(preds, views, niter_PnP=10):\n",
    "    \"\"\"Estimate camera poses and focal lengths using fast_pnp.\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    poses_c2w = []  # List of camera-to-world poses\n",
    "    estimated_focals = []  # List of guessed focal lengths\n",
    "\n",
    "    for view_idx in range(len(preds)):\n",
    "        pts3d = preds[view_idx][\"pts3d_in_other_view\"].cpu().numpy().squeeze()  # (224, 224, 3) shape\n",
    "        valid_mask = preds[view_idx][\"conf\"].cpu().numpy().squeeze() > 0.5  # Confidence mask\n",
    "        img_rgb = views[view_idx]['img'].cpu().numpy().squeeze().transpose(1, 2, 0)  # (224, 224, 3)\n",
    "\n",
    "        # Call fast_pnp with unflattened pts3d and mask\n",
    "        focal_length, pose_c2w = fast_pnp(\n",
    "            torch.tensor(pts3d, device=device),  # Unmasked points\n",
    "            None,  # Guess focal length\n",
    "            torch.tensor(valid_mask, device=device, dtype=torch.bool),  # Valid mask (unflattened)\n",
    "            device,\n",
    "            pp=None,  # Use default principal point (center of image)\n",
    "            niter_PnP=niter_PnP\n",
    "        )\n",
    "\n",
    "        if pose_c2w is None:\n",
    "            print(f\"Failed to estimate pose for view {view_idx}\")\n",
    "            continue\n",
    "\n",
    "        # Store estimated camera-to-world pose and focal length\n",
    "        poses_c2w.append(pose_c2w.cpu().numpy())\n",
    "        estimated_focals.append(focal_length)\n",
    "\n",
    "    return poses_c2w, estimated_focals\n",
    "\n",
    "# Function to visualize 3D points and camera poses with SceneViz\n",
    "def plot_3d_points_with_estimated_camera_poses(preds, views, title=\"3D Points and Camera Poses\", flip_axes=False, min_conf_thr_percentile=80, export_ply_path=None):\n",
    "    # Initialize SceneViz for visualization\n",
    "    viz = SceneViz()\n",
    "\n",
    "    # Flip axes if requested\n",
    "    if flip_axes:\n",
    "        preds = copy.deepcopy(preds)\n",
    "        for i, pred in enumerate(preds):\n",
    "            pts3d = pred['pts3d_in_other_view'].cpu().numpy().squeeze()  # (224, 224, 3)\n",
    "            pts3d = pts3d[..., [0, 2, 1]]  # Swap Y and Z axes\n",
    "            pts3d[..., 2] *= -1  # Flip the sign of the Z axis\n",
    "            pred['pts3d_in_other_view'] = torch.tensor(pts3d)  # Reassign the modified points back to pred\n",
    "\n",
    "    # Estimate camera poses and focal lengths\n",
    "    poses_c2w, estimated_focals = estimate_camera_poses(preds, views, niter_PnP=10)\n",
    "    cam_size = max(auto_cam_size(poses_c2w), 0.05)  # Auto-scale based on the point cloud\n",
    "\n",
    "    # Set up point clouds and visualization\n",
    "    for i, (pred, pose_c2w) in enumerate(zip(preds, poses_c2w)):\n",
    "        pts3d = pred['pts3d_in_other_view'].cpu().numpy().squeeze()  # (224, 224, 3)\n",
    "        img_rgb = rgb(views[i]['img'].cpu().numpy().squeeze().transpose(1, 2, 0))  # Shape: (224, 224, 3)\n",
    "        conf = pred['conf'].cpu().numpy().squeeze()\n",
    "\n",
    "        # Determine the confidence threshold based on the percentile\n",
    "        conf_thr = np.percentile(conf, min_conf_thr_percentile)\n",
    "        mask = conf > conf_thr\n",
    "\n",
    "        # Add the point cloud directly to the SceneViz object\n",
    "        viz.add_pointcloud(pts3d, img_rgb, mask=mask, point_size=2, view_idx=i)\n",
    "\n",
    "        # Add camera to the visualization\n",
    "        viz.add_camera(\n",
    "            pose_c2w=pose_c2w,  # Estimated camera-to-world pose\n",
    "            focal=estimated_focals[i],  # Estimated focal length for each view\n",
    "            color=np.random.randint(0, 256, size=3),  # Generate a random RGB color for each camera\n",
    "            image=img_rgb,  # Image of the view\n",
    "            cam_size=cam_size,  # Auto-scaled camera size\n",
    "            view_idx=i\n",
    "        )\n",
    "\n",
    "    # Export point clouds and meshes if the path is provided\n",
    "    if export_ply_path:\n",
    "        all_points = []\n",
    "        all_colors = []\n",
    "        for i, pred in enumerate(preds):\n",
    "            pts3d = pred['pts3d_in_other_view'].cpu().numpy().squeeze()\n",
    "            img_rgb = views[i]['img'].cpu().numpy().squeeze().transpose(1, 2, 0)\n",
    "            conf = pred['conf'].cpu().numpy().squeeze()\n",
    "            conf_thr = np.percentile(conf, min_conf_thr_percentile)\n",
    "            mask = conf > conf_thr\n",
    "            all_points.append(pts3d[mask])\n",
    "            all_colors.append(img_rgb[mask])\n",
    "        \n",
    "        all_points = np.vstack(all_points)\n",
    "        all_colors = np.vstack(all_colors)\n",
    "        point_cloud = trimesh.PointCloud(vertices=all_points, colors=all_colors)\n",
    "        point_cloud.export(export_ply_path)\n",
    "\n",
    "    # Show the visualization\n",
    "    viz.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "data_root = \"/opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/jianingy/research/accel-cortex/dust3r/data\"\n",
    "\n",
    "filelist_train = [\n",
    "    f\"{data_root}/co3d_subset_processed/apple/189_20393_38136/images/frame000001.jpg\",\n",
    "    f\"{data_root}/co3d_subset_processed/apple/189_20393_38136/images/frame000002.jpg\"\n",
    "]\n",
    "\n",
    "# apple\n",
    "filelist_test = [\n",
    "    f\"{data_root}/co3d_subset_processed/apple/189_20393_38136/images/frame000200.jpg\",\n",
    "    f\"{data_root}/co3d_subset_processed/apple/189_20393_38136/images/frame000085.jpg\",\n",
    "    f\"{data_root}/co3d_subset_processed/apple/189_20393_38136/images/frame000090.jpg\",\n",
    "    f\"{data_root}/co3d_subset_processed/apple/189_20393_38136/images/frame000170.jpg\",\n",
    "    f\"{data_root}/co3d_subset_processed/apple/189_20393_38136/images/frame000199.jpg\",\n",
    "]\n",
    "\n",
    "\n",
    "# bench test\n",
    "# filelist_test = [\n",
    "#     f\"{data_root}/co3d_subset_processed/bench/415_57112_110099/images/frame000006.jpg\",\n",
    "#     f\"{data_root}/co3d_subset_processed/bench/415_57112_110099/images/frame000016.jpg\",\n",
    "#     f\"{data_root}/co3d_subset_processed/bench/415_57112_110099/images/frame000026.jpg\",\n",
    "#     f\"{data_root}/co3d_subset_processed/bench/415_57112_110099/images/frame000036.jpg\",\n",
    "#     f\"{data_root}/co3d_subset_processed/bench/415_57112_110099/images/frame000096.jpg\",\n",
    "#     f\"{data_root}/co3d_subset_processed/bench/415_57112_110099/images/frame000126.jpg\",\n",
    "#     f\"{data_root}/co3d_subset_processed/bench/415_57112_110099/images/frame000156.jpg\",\n",
    "#     f\"{data_root}/co3d_subset_processed/bench/415_57112_110099/images/frame000186.jpg\",\n",
    "# ]\n",
    "\n",
    "# # teddy bear train\n",
    "# filelist_test = [\n",
    "#     f\"{data_root}/co3d_subset_processed/teddybear/34_1479_4753/images/frame000001.jpg\",\n",
    "#     f\"{data_root}/co3d_subset_processed/teddybear/34_1479_4753/images/frame000002.jpg\",\n",
    "#     f\"{data_root}/co3d_subset_processed/teddybear/34_1479_4753/images/frame000003.jpg\",\n",
    "#     f\"{data_root}/co3d_subset_processed/teddybear/34_1479_4753/images/frame000004.jpg\",\n",
    "#     # f\"{data_root}/co3d_subset_processed/teddybear/34_1479_4753/images/frame000012.jpg\",\n",
    "#     # f\"{data_root}/co3d_subset_processed/teddybear/34_1479_4753/images/frame000022.jpg\",\n",
    "#     # f\"{data_root}/co3d_subset_processed/teddybear/34_1479_4753/images/frame000032.jpg\",\n",
    "# ]\n",
    "# teddy bear test\n",
    "# filelist_test = [\n",
    "#     f\"{data_root}/co3d_subset_processed/teddybear/34_1479_4753/images/frame000006.jpg\",\n",
    "#     f\"{data_root}/co3d_subset_processed/teddybear/34_1479_4753/images/frame000016.jpg\",\n",
    "#     f\"{data_root}/co3d_subset_processed/teddybear/34_1479_4753/images/frame000026.jpg\",\n",
    "#     f\"{data_root}/co3d_subset_processed/teddybear/34_1479_4753/images/frame000036.jpg\",\n",
    "#     f\"{data_root}/co3d_subset_processed/teddybear/34_1479_4753/images/frame000096.jpg\",\n",
    "#     f\"{data_root}/co3d_subset_processed/teddybear/34_1479_4753/images/frame000126.jpg\",\n",
    "#     f\"{data_root}/co3d_subset_processed/teddybear/34_1479_4753/images/frame000156.jpg\",\n",
    "#     f\"{data_root}/co3d_subset_processed/teddybear/34_1479_4753/images/frame000186.jpg\",\n",
    "# ]\n",
    "\n",
    "# teddy bear random order\n",
    "# filelist_test = [\n",
    "#     f\"{data_root}/co3d_subset_processed/teddybear/34_1479_4753/images/frame000126.jpg\",\n",
    "#     f\"{data_root}/co3d_subset_processed/teddybear/34_1479_4753/images/frame000036.jpg\",\n",
    "#     f\"{data_root}/co3d_subset_processed/teddybear/34_1479_4753/images/frame000096.jpg\",\n",
    "#     f\"{data_root}/co3d_subset_processed/teddybear/34_1479_4753/images/frame000006.jpg\",\n",
    "#     f\"{data_root}/co3d_subset_processed/teddybear/34_1479_4753/images/frame000026.jpg\",\n",
    "#     f\"{data_root}/co3d_subset_processed/teddybear/34_1479_4753/images/frame000186.jpg\",\n",
    "#     f\"{data_root}/co3d_subset_processed/teddybear/34_1479_4753/images/frame000016.jpg\",\n",
    "#     f\"{data_root}/co3d_subset_processed/teddybear/34_1479_4753/images/frame000156.jpg\",\n",
    "# ]\n",
    "\n",
    "\n",
    "# filelist_test = [\n",
    "#     f\"{data_root}/co3d_subset_processed/teddybear/34_1479_4753/images/frame000006.jpg\",\n",
    "#     f\"{data_root}/co3d_subset_processed/teddybear/34_1479_4753/images/frame000036.jpg\",\n",
    "#     f\"{data_root}/co3d_subset_processed/teddybear/34_1479_4753/images/frame000066.jpg\",\n",
    "#     f\"{data_root}/co3d_subset_processed/teddybear/34_1479_4753/images/frame000096.jpg\",\n",
    "# ]\n",
    "\n",
    "# suitcase test\n",
    "# filelist_test = [\n",
    "#     f\"{data_root}/co3d_subset_processed/suitcase/50_2928_8645/images/frame000006.jpg\",\n",
    "#     f\"{data_root}/co3d_subset_processed/suitcase/50_2928_8645/images/frame000016.jpg\",\n",
    "#     f\"{data_root}/co3d_subset_processed/suitcase/50_2928_8645/images/frame000026.jpg\",\n",
    "#     f\"{data_root}/co3d_subset_processed/suitcase/50_2928_8645/images/frame000036.jpg\",\n",
    "#     f\"{data_root}/co3d_subset_processed/suitcase/50_2928_8645/images/frame000096.jpg\",\n",
    "#     f\"{data_root}/co3d_subset_processed/suitcase/50_2928_8645/images/frame000126.jpg\",\n",
    "#     f\"{data_root}/co3d_subset_processed/suitcase/50_2928_8645/images/frame000156.jpg\",\n",
    "#     f\"{data_root}/co3d_subset_processed/suitcase/50_2928_8645/images/frame000186.jpg\",\n",
    "# ]\n",
    "\n",
    "# cake test\n",
    "# filelist_test = [\n",
    "#     f\"{data_root}/co3d_subset_processed/cake/374_42274_84517/images/frame000006.jpg\",\n",
    "#     f\"{data_root}/co3d_subset_processed/cake/374_42274_84517/images/frame000016.jpg\",\n",
    "#     f\"{data_root}/co3d_subset_processed/cake/374_42274_84517/images/frame000026.jpg\",\n",
    "#     f\"{data_root}/co3d_subset_processed/cake/374_42274_84517/images/frame000036.jpg\",\n",
    "#     f\"{data_root}/co3d_subset_processed/cake/374_42274_84517/images/frame000096.jpg\",\n",
    "#     f\"{data_root}/co3d_subset_processed/cake/374_42274_84517/images/frame000126.jpg\",\n",
    "#     f\"{data_root}/co3d_subset_processed/cake/374_42274_84517/images/frame000156.jpg\",\n",
    "#     f\"{data_root}/co3d_subset_processed/cake/374_42274_84517/images/frame000186.jpg\",\n",
    "# ]\n",
    "\n",
    "# in-the-wild obj: book\n",
    "# filelist_test = [\n",
    "#     f\"{data_root}/unseen_book/IMG_9837.jpg\",\n",
    "#     f\"{data_root}/unseen_book/IMG_9838.jpg\",\n",
    "#     f\"{data_root}/unseen_book/IMG_9839.jpg\",\n",
    "#     f\"{data_root}/unseen_book/IMG_9840.jpg\",\n",
    "#     f\"{data_root}/unseen_book/IMG_9841.jpg\",\n",
    "#     f\"{data_root}/unseen_book/IMG_9842.jpg\",\n",
    "#     f\"{data_root}/unseen_book/IMG_9843.jpg\",\n",
    "#     f\"{data_root}/unseen_book/IMG_9844.jpg\",\n",
    "# ]\n",
    "\n",
    "# in-the-wild obj: beef jerky\n",
    "# filelist_test = [\n",
    "#     f\"{data_root}/beef_jerky/IMG_0050.jpg\",\n",
    "#     f\"{data_root}/beef_jerky/IMG_0051.jpg\",\n",
    "#     f\"{data_root}/beef_jerky/IMG_0052.jpg\",\n",
    "#     f\"{data_root}/beef_jerky/IMG_0053.jpg\",\n",
    "#     f\"{data_root}/beef_jerky/IMG_0054.jpg\",\n",
    "#     f\"{data_root}/beef_jerky/IMG_0055.jpg\",\n",
    "#     f\"{data_root}/beef_jerky/IMG_0056.jpg\",\n",
    "#     f\"{data_root}/beef_jerky/IMG_0057.jpg\",\n",
    "#     f\"{data_root}/beef_jerky/IMG_0058.jpg\",\n",
    "# ]\n",
    "\n",
    "# HSSD\n",
    "# filelist_test = [\n",
    "#     f\"{data_root}/0_102344022_0/rgb/0000{i:02d}.png\" for i in range(8)\n",
    "# ]\n",
    "\n",
    "filelist_test = [\n",
    "    f\"{data_root}/17_102344250_4/rgb/0000{i:02d}.png\" for i in range(2,8)\n",
    "]\n",
    "\n",
    "# unseen obj: teddy bear from co3d\n",
    "# filelist_test = [\n",
    "#     \"/datasets01/co3dv2/080422/teddybear/595_90395_180050/images/frame000006.jpg\",\n",
    "#     \"/datasets01/co3dv2/080422/teddybear/595_90395_180050/images/frame000036.jpg\",\n",
    "#     \"/datasets01/co3dv2/080422/teddybear/595_90395_180050/images/frame000056.jpg\",\n",
    "#     \"/datasets01/co3dv2/080422/teddybear/595_90395_180050/images/frame000086.jpg\",\n",
    "#     \"/datasets01/co3dv2/080422/teddybear/595_90395_180050/images/frame000096.jpg\",\n",
    "#     \"/datasets01/co3dv2/080422/teddybear/595_90395_180050/images/frame000126.jpg\",\n",
    "#     \"/datasets01/co3dv2/080422/teddybear/595_90395_180050/images/frame000156.jpg\",\n",
    "#     \"/datasets01/co3dv2/080422/teddybear/595_90395_180050/images/frame000186.jpg\",\n",
    "# ]\n",
    "\n",
    "\n",
    "# unseen obj: keyboard from co3d\n",
    "# filelist_test = [\n",
    "#     \"/datasets01/co3dv2/080422/keyboard/604_93822_187288/images/frame000096.jpg\",\n",
    "#     \"/datasets01/co3dv2/080422/keyboard/604_93822_187288/images/frame000126.jpg\",\n",
    "#     \"/datasets01/co3dv2/080422/keyboard/604_93822_187288/images/frame000156.jpg\",\n",
    "#     # \"/datasets01/co3dv2/080422/keyboard/604_93822_187288/images/frame000186.jpg\",\n",
    "#     # \"/datasets01/co3dv2/080422/keyboard/604_93822_187288/images/frame000006.jpg\",\n",
    "#     # \"/datasets01/co3dv2/080422/keyboard/604_93822_187288/images/frame000016.jpg\",\n",
    "#     # \"/datasets01/co3dv2/080422/keyboard/604_93822_187288/images/frame000026.jpg\",\n",
    "#     \"/datasets01/co3dv2/080422/keyboard/604_93822_187288/images/frame000036.jpg\",\n",
    "# ]\n",
    "\n",
    "# filelist_test = [\n",
    "#     \"/fsx-cortex/jianingy/dust3r_data/co3d_50_seqs_per_category_subset_processed/keyboard/76_7733_16196/images/frame000006.jpg\",\n",
    "#     \"/fsx-cortex/jianingy/dust3r_data/co3d_50_seqs_per_category_subset_processed/keyboard/76_7733_16196/images/frame000016.jpg\",\n",
    "#     \"/fsx-cortex/jianingy/dust3r_data/co3d_50_seqs_per_category_subset_processed/keyboard/76_7733_16196/images/frame000026.jpg\",\n",
    "#     \"/fsx-cortex/jianingy/dust3r_data/co3d_50_seqs_per_category_subset_processed/keyboard/76_7733_16196/images/frame000036.jpg\",\n",
    "#     \"/fsx-cortex/jianingy/dust3r_data/co3d_50_seqs_per_category_subset_processed/keyboard/76_7733_16196/images/frame000096.jpg\",\n",
    "#     \"/fsx-cortex/jianingy/dust3r_data/co3d_50_seqs_per_category_subset_processed/keyboard/76_7733_16196/images/frame000126.jpg\",\n",
    "#     \"/fsx-cortex/jianingy/dust3r_data/co3d_50_seqs_per_category_subset_processed/keyboard/76_7733_16196/images/frame000156.jpg\",\n",
    "#     \"/fsx-cortex/jianingy/dust3r_data/co3d_50_seqs_per_category_subset_processed/keyboard/76_7733_16196/images/frame000186.jpg\",\n",
    "# ]\n",
    "\n",
    "# DTU\n",
    "# filelist_test = [\n",
    "#     \"/fsx-cortex/jianingy/dust3r_data/datasets_raw/DTU/SampleSet/MVS Data/Rectified/scan6/rect_001_max.png\",\n",
    "#     \"/fsx-cortex/jianingy/dust3r_data/datasets_raw/DTU/SampleSet/MVS Data/Rectified/scan6/rect_002_max.png\",\n",
    "#     \"/fsx-cortex/jianingy/dust3r_data/datasets_raw/DTU/SampleSet/MVS Data/Rectified/scan6/rect_003_max.png\",\n",
    "#     \"/fsx-cortex/jianingy/dust3r_data/datasets_raw/DTU/SampleSet/MVS Data/Rectified/scan6/rect_004_max.png\",\n",
    "#     \"/fsx-cortex/jianingy/dust3r_data/datasets_raw/DTU/SampleSet/MVS Data/Rectified/scan6/rect_005_max.png\",\n",
    "#     \"/fsx-cortex/jianingy/dust3r_data/datasets_raw/DTU/SampleSet/MVS Data/Rectified/scan6/rect_006_max.png\",\n",
    "#     \"/fsx-cortex/jianingy/dust3r_data/datasets_raw/DTU/SampleSet/MVS Data/Rectified/scan6/rect_007_max.png\",\n",
    "#     \"/fsx-cortex/jianingy/dust3r_data/datasets_raw/DTU/SampleSet/MVS Data/Rectified/scan6/rect_008_max.png\",\n",
    "# ]\n",
    "\n",
    "# display the images\n",
    "def display_images(filelist, title):\n",
    "    fig, axes = plt.subplots(1, len(filelist), figsize=(30, 4))\n",
    "    fig.suptitle(title)\n",
    "    for ax, filepath in zip(axes if hasattr(axes, '__iter__') else [axes], filelist):\n",
    "        img = Image.open(filepath)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# # Display train images\n",
    "# display_images(filelist_train, 'Train Images')\n",
    "\n",
    "# Display test images\n",
    "display_images(filelist_test, 'Test Images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "checkpoint_root = \"/opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/jianingy/research/accel-cortex/dust3r/checkpoints\"\n",
    "\n",
    "# model = FlashDUSt3R.from_pretrained(f'{checkpoint_root}/dust3r_demo_224_longer_epochs/checkpoint-best.pth').to(device)\n",
    "# model = FlashDUSt3R.from_pretrained(f'{checkpoint_root}/dust3r_demo_224_multiview/checkpoint-best.pth').to(device)\n",
    "# model = FlashDUSt3R.from_pretrained(f'{checkpoint_root}/dust3r_demo_224_multiview_co3d_full/checkpoint-last.pth').to(device)\n",
    "# model = FlashDUSt3R.from_pretrained(f'{checkpoint_root}/dust3r_demo_224_multiview_co3d_full_100_epochs_100_samples_per_window/checkpoint-last.pth').to(device)\n",
    "# model = FlashDUSt3R.from_pretrained(f'{checkpoint_root}/dust3r_512_dpt_finetune_multiview_co3d_50_seqs_per_cat_subset/checkpoint-best.pth').to(device)\n",
    "# model = FlashDUSt3R.from_pretrained(f'{checkpoint_root}/dust3r_512_dpt_finetune_multiview_co3d_50_seqs_per_cat_subset_100_epochs_unfreeze_dec_and_head/checkpoint-last.pth').to(device)\n",
    "# model = FlashDUSt3R.from_pretrained(f'{checkpoint_root}/dust3r_512_dpt_finetune_multiview_co3d_50_seqs_per_cat_subset_100_epochs_unfreeze_everything/checkpoint-last.pth').to(device)\n",
    "# model = FlashDUSt3R.from_pretrained(f'{checkpoint_root}/dust3r_512_dpt_finetune_multiview_co3d_50_seqs_per_cat_subset_100_epochs_unfreeze_everything_large/checkpoint-last.pth').to(device)\n",
    "# model = FlashDUSt3R.from_pretrained(f'{checkpoint_root}/dust3r_512_dpt_finetune_multiview_co3d_50_seqs_per_cat_subset_100_epochs_unfreeze_everything_co3d_scannetpp_megadepth/checkpoint-last.pth').to(device)\n",
    "model = FlashDUSt3R.from_pretrained(f'{checkpoint_root}/dust3r_512_dpt_finetune_multiview_co3d_50_seqs_per_cat_subset_100_epochs_unfreeze_everything_co3d_scannetpp_megadepth_large/checkpoint-last.pth').to(device)\n",
    "# model = FlashDUSt3R.from_pretrained(f'{checkpoint_root}/dust3r_512_dpt_finetune_multiview_co3d_50_seqs_per_cat_subset_100_epochs_unfreeze_everything_co3d_scannetpp_megadepth_large/checkpoint-last.pth').to(device)\n",
    "# model = FlashDUSt3R.from_pretrained(f'{checkpoint_root}/bf16_flash_attn_unfreeze_everything_co3d_scannetpp_megadepth_large_bs4/checkpoint-10.pth').to(device)\n",
    "# model = FlashDUSt3R.from_pretrained(f'{checkpoint_root}/bf16_flash_attn_unfreeze_everything_co3d_scannetpp_megadepth_large/checkpoint-last.pth').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightning model\n",
    "# instantiate lit module from hydra yaml /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/jianingy/research/accel-cortex/dust3r/fast3r/configs/model/multiview_dust3r.yaml\n",
    "device = torch.device(\"cuda\")\n",
    "checkpoint_root = \"/opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/jianingy/research/accel-cortex\"\n",
    "\n",
    "# checkpoint_dir = f\"{checkpoint_root}/dust3r/fast3r/logs/train/runs/2024-08-13_04-40-37\"  #fp32-fancy-sun-181\n",
    "# checkpoint_dir = f\"{checkpoint_root}/dust3r/fast3r/logs/train/runs/2024-08-13_08-06-08\"  #fp32_workers11_giddy-gorge-182\n",
    "# checkpoint_dir = f\"{checkpoint_root}/dust3r/fast3r/logs/fp32_bs6_views4/runs/fp32_bs6_views4_3782640\"\n",
    "# checkpoint_dir = f\"{checkpoint_root}/dust3r/fast3r/logs/fp32_bs2_views8/runs/fp32_bs2_views8_3782638\"\n",
    "# checkpoint_dir = f\"{checkpoint_root}/dust3r/fast3r/logs/fp32_bs6_views4/runs/fp32_bs6_views4_4007485\"  # with random image idx embeddings\n",
    "# checkpoint_dir = f\"{checkpoint_root}/dust3r/fast3r/logs/fp32_bs6_views4/runs/fp32_bs6_views4_4030983\"  # fix Regr3D loss (wrong rotation)\n",
    "# checkpoint_dir = f\"{checkpoint_root}/dust3r/fast3r/logs/fp32_bs6_views4/runs/fp32_bs6_views4_4037511\"  # fix Regr3D loss (fixed rotation)\n",
    "# checkpoint_dir = f\"{checkpoint_root}/dust3r/fast3r/logs/fp32_bs6_views4_scannetpp_only/runs/fp32_bs6_views4_scannetpp_only_4060428\"  # ScanNet++ only no random emb\n",
    "# checkpoint_dir = f\"{checkpoint_root}/dust3r/fast3r/logs/fp32_bs6_views4_scannetpp_only/runs/fp32_bs6_views4_scannetpp_only_4051504\"  # ScanNet++ only\n",
    "# checkpoint_dir = f\"{checkpoint_root}/dust3r/fast3r/logs/fp32_bs6_views4_arkitscenes_only/runs/arkitscenes_only_4123064\"  # ARKitScenes only\n",
    "# checkpoint_dir = f\"{checkpoint_root}/dust3r/fast3r/logs/arkitscenes_only_no_pairs/runs/arkitscenes_only_no_pairs_4129400\"  # ARKitScenes only no pairs\n",
    "checkpoint_dir = f\"{checkpoint_root}/dust3r/fast3r/logs/co3d_scannetpp/runs/co3d_scannetpp_4123062\"  # co3d_scannetpp\n",
    "# checkpoint_dir = f\"{checkpoint_root}/dust3r/fast3r/logs/co3d_scannetpp_arkitscenes/runs/co3d_scannetpp_arkitscenes_4123063\"  # co3d_scannetpp_arkitscenes\n",
    "\n",
    "print(\"Creating an empty lightning module to hold the weights...\")\n",
    "cfg = OmegaConf.load(os.path.join(checkpoint_dir, '.hydra/config.yaml'))\n",
    "\n",
    "# replace all occurances of \"dust3r.\" in cfg.model.net with \"src.dust3r.\" (this is due to relocation of our code)\n",
    "def replace_dust3r_in_config(cfg):\n",
    "    for key, value in cfg.items():\n",
    "        if isinstance(value, dict):\n",
    "            replace_dust3r_in_config(value)\n",
    "        elif isinstance(value, str):\n",
    "            if \"dust3r.\" in value and \"src.dust3r.\" not in value:\n",
    "                cfg[key] = value.replace(\"dust3r.\", \"src.dust3r.\")\n",
    "    return cfg\n",
    "\n",
    "\n",
    "cfg.model.net = replace_dust3r_in_config(cfg.model.net)\n",
    "\n",
    "cfg.model.net.patch_embed_cls = \"PatchEmbedDust3R\"  # TODO: investigate what exactly this does, this seems to support inferencing images of protrait orientation\n",
    "cfg.model.net.landscape_only = False  # TODO: investigate what exactly this does\n",
    "\n",
    "\n",
    "empty_lit_module = hydra.utils.instantiate(cfg.model, train_criterion=None, validation_criterion=None)\n",
    "\n",
    "\n",
    "print(\"Loading weights from checkpoint...\")\n",
    "CKPT_PATH = os.path.join(checkpoint_dir, 'checkpoints/last.ckpt')\n",
    "\n",
    "checkpoint = torch.load(CKPT_PATH)\n",
    "empty_lit_module.load_state_dict(checkpoint[\"state_dict\"])\n",
    "model = empty_lit_module.net.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = get_reconstructed_scene(\n",
    "    outdir = \"./output\",\n",
    "    model = model,\n",
    "    device = device,\n",
    "    silent = False,\n",
    "    # image_size = 224,\n",
    "    image_size = 512,\n",
    "    filelist = filelist_test,\n",
    "    dtype = torch.float32,\n",
    "    # dtype = torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "# Usage example in your context\n",
    "# Plot the RGB images\n",
    "plot_rgb_images(output['views'])\n",
    "\n",
    "# Plot the confidence maps\n",
    "plot_confidence_maps(output['preds'])\n",
    "\n",
    "# Plot the 3D points along with estimated camera poses\n",
    "plot_3d_points_with_estimated_camera_poses(\n",
    "    output['preds'],  # Predictions containing 3D points\n",
    "    output['views'],  # Views containing RGB images\n",
    "    flip_axes=True,   # Enable flipping of axes (swap Y and Z and flip Z)\n",
    "    min_conf_thr_percentile=40,  # Confidence threshold percentile for filtering points\n",
    "    export_ply_path='./output/combined_mesh.ply'  # Export path for the .ply file\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the RGB images\n",
    "plot_rgb_images(output['views'])\n",
    "\n",
    "# Plot the confidence maps\n",
    "plot_confidence_maps(output['preds'])\n",
    "\n",
    "# Plot the 3D points\n",
    "plot_3d_points_with_colors(output['preds'], output['views'], flip_axes=True, as_mesh=False, min_conf_thr_percentile=30, export_ply_path='./output/combined_mesh.ply')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from scipy.linalg import rq\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def estimate_camera_matrix(world_points, image_points):\n",
    "    \"\"\"\n",
    "    Estimate the camera matrix from 3D world points and 2D image points using DLT.\n",
    "    \n",
    "    Parameters:\n",
    "    world_points (np.ndarray): Array of 3D points in the world coordinates, shape (N, 3).\n",
    "    image_points (np.ndarray): Array of 2D points in the image coordinates, shape (N, 2).\n",
    "    \n",
    "    Returns:\n",
    "    np.ndarray: The 3x4 camera matrix.\n",
    "    \"\"\"\n",
    "    assert world_points.shape[0] == image_points.shape[0], \"Number of points must match\"\n",
    "    num_points = world_points.shape[0]\n",
    "    \n",
    "    # Add homogeneous coordinates to the world points\n",
    "    homogeneous_world_points = np.hstack((world_points, np.ones((num_points, 1))))\n",
    "    \n",
    "    A = []\n",
    "    \n",
    "    for i in range(num_points):\n",
    "        X, Y, Z, _ = homogeneous_world_points[i]\n",
    "        u, v = image_points[i]\n",
    "        \n",
    "        # Two rows of the equation for each point\n",
    "        A.append([X, Y, Z, 1, 0, 0, 0, 0, -u*X, -u*Y, -u*Z, -u])\n",
    "        A.append([0, 0, 0, 0, X, Y, Z, 1, -v*X, -v*Y, -v*Z, -v])\n",
    "    \n",
    "    # Convert A to a numpy array\n",
    "    A = np.array(A)\n",
    "    \n",
    "    # Solve using SVD (Singular Value Decomposition)\n",
    "    U, S, Vt = np.linalg.svd(A)\n",
    "    \n",
    "    # The last row of Vt (or last column of V) is the solution\n",
    "    P = Vt[-1].reshape(3, 4)\n",
    "    \n",
    "    return P\n",
    "\n",
    "def decompose_camera_matrix(P):\n",
    "    \"\"\"\n",
    "    Decompose the camera matrix into intrinsic and extrinsic matrices.\n",
    "    \n",
    "    Parameters:\n",
    "    P (np.ndarray): The 3x4 camera matrix.\n",
    "    \n",
    "    Returns:\n",
    "    K (np.ndarray): The 3x3 intrinsic matrix.\n",
    "    R (np.ndarray): The 3x3 rotation matrix.\n",
    "    t (np.ndarray): The 3x1 translation vector.\n",
    "    \"\"\"\n",
    "    # Extract the camera matrix K and rotation matrix R using RQ decomposition\n",
    "    M = P[:, :3]  # The first 3x3 part of P\n",
    "    \n",
    "    # RQ Decomposition of M\n",
    "    K, R = rq(M)\n",
    "    \n",
    "    # Normalize K so that K[2,2] = 1\n",
    "    K /= K[2, 2]\n",
    "    \n",
    "    # Compute translation vector\n",
    "    t = np.dot(np.linalg.inv(K), P[:, 3])\n",
    "    \n",
    "    return K, R, t\n",
    "\n",
    "def plot_camera_cones(fig, R, t, K, color='blue', scale=0.1):\n",
    "    \"\"\"\n",
    "    Plot the camera as a cone in 3D space based on the intrinsic matrix K for focal length.\n",
    "    \n",
    "    Parameters:\n",
    "    fig (plotly.graph_objects.Figure): The existing Plotly figure.\n",
    "    R (np.ndarray): The 3x3 rotation matrix.\n",
    "    t (np.ndarray): The 3x1 translation vector.\n",
    "    K (np.ndarray): The 3x3 intrinsic matrix.\n",
    "    color (str): Color of the camera cone.\n",
    "    scale (float): Scale factor for the size of the cone base.\n",
    "    \"\"\"\n",
    "    # The focal length is the element K[0, 0] (assuming fx and fy are equal)\n",
    "    focal_length = K[0, 0] / K[2, 2]\n",
    "\n",
    "    # The camera center (apex of the cone)\n",
    "    camera_center = -R.T @ t\n",
    "\n",
    "    # Define the orientation of the cone based on the rotation matrix\n",
    "    direction = R.T @ np.array([0, 0, 1])  # Camera looks along the +Z axis in camera space\n",
    "\n",
    "    # Scale the direction by the focal length\n",
    "    direction = direction * focal_length\n",
    "\n",
    "    # Plot the camera cone\n",
    "    fig.add_trace(go.Cone(\n",
    "        x=[camera_center[0]],\n",
    "        y=[camera_center[1]],\n",
    "        z=[camera_center[2]],\n",
    "        u=[direction[0]],\n",
    "        v=[direction[1]],\n",
    "        w=[direction[2]],\n",
    "        colorscale=[[0, color], [1, color]],  # Single color for the cone\n",
    "        showscale=False,\n",
    "        sizemode=\"absolute\",\n",
    "        sizeref=scale,  # The size of the cone base\n",
    "        anchor=\"tip\",  # The tip of the cone is the camera center\n",
    "        name=\"Camera Cone\"\n",
    "    ))\n",
    "\n",
    "def plot_3d_points_with_estimated_camera(output, fig, camera_poses, min_conf_thr_percentile=80):\n",
    "    \"\"\"\n",
    "    Plot 3D points together with estimated camera cones in the same plot.\n",
    "    \n",
    "    Parameters:\n",
    "    output (dict): The output containing 'preds' with 3D points and corresponding 2D image points.\n",
    "    fig (plotly.graph_objects.Figure): The existing 3D plot.\n",
    "    camera_poses (list): List of estimated camera poses.\n",
    "    min_conf_thr_percentile (int): Percentile threshold for confidence values to filter points.\n",
    "    \"\"\"\n",
    "    # Plot the 3D points first\n",
    "    all_points = []\n",
    "    all_colors = []\n",
    "\n",
    "    for i, pred in enumerate(output['preds']):\n",
    "        pts3d = pred['pts3d_in_other_view'].cpu().numpy().squeeze()  # 3D points\n",
    "        img_rgb = output['views'][i]['img'].cpu().numpy().squeeze().transpose(1, 2, 0)  # RGB image (224x224)\n",
    "        conf = pred['conf'].cpu().numpy().squeeze()  # Confidence map\n",
    "\n",
    "        # Apply confidence threshold\n",
    "        conf_thr = np.percentile(conf, min_conf_thr_percentile)\n",
    "        mask = conf > conf_thr\n",
    "\n",
    "        # Rescale RGB values from [-1, 1] to [0, 255]\n",
    "        img_rgb = ((img_rgb + 1) * 127.5).astype(np.uint8).clip(0, 255)\n",
    "\n",
    "        # Flatten the points and colors, and apply mask\n",
    "        x, y, z = pts3d[..., 0].flatten(), pts3d[..., 1].flatten(), pts3d[..., 2].flatten()\n",
    "        r, g, b = img_rgb[..., 0].flatten(), img_rgb[..., 1].flatten(), img_rgb[..., 2].flatten()\n",
    "        x, y, z = x[mask.flatten()], y[mask.flatten()], z[mask.flatten()]\n",
    "        r, g, b = r[mask.flatten()], g[mask.flatten()], b[mask.flatten()]\n",
    "\n",
    "        colors = ['rgb({}, {}, {})'.format(r[j], g[j], b[j]) for j in range(len(r))]\n",
    "\n",
    "        # Add points to the plot\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=x, y=y, z=z,\n",
    "            mode='markers',\n",
    "            marker=dict(size=2, opacity=0.8, color=colors),\n",
    "            name=f\"View {i} Points\"\n",
    "        ))\n",
    "\n",
    "    # Now, plot the estimated cameras as cones\n",
    "    for i, (R, t, K) in enumerate(camera_poses):\n",
    "        plot_camera_cones(fig, R, t, K, color='blue')\n",
    "\n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis_title='X',\n",
    "            yaxis_title='Y',\n",
    "            zaxis_title='Z'\n",
    "        ),\n",
    "        margin=dict(l=0, r=0, b=0, t=40)\n",
    "    )\n",
    "\n",
    "def estimate_camera_poses(output, min_conf_thr_percentile=80):\n",
    "    \"\"\"\n",
    "    Estimate camera poses from 3D points and 2D image points.\n",
    "    \n",
    "    Parameters:\n",
    "    output (dict): The output containing 'preds' with 3D points and corresponding 2D image points.\n",
    "    min_conf_thr_percentile (int): Percentile threshold for confidence values to filter points.\n",
    "    \n",
    "    Returns:\n",
    "    list: A list of camera poses (R, t, K) where R is rotation, t is translation, and K is intrinsic matrix.\n",
    "    \"\"\"\n",
    "    camera_poses = []\n",
    "    \n",
    "    # Loop through all views in output['preds']\n",
    "    for i, pred in enumerate(output['preds']):\n",
    "        # Get the 3D points and confidence map for the current view\n",
    "        world_points = pred['pts3d_in_other_view'].cpu().numpy().squeeze()  # Shape: (272, 512, 3)\n",
    "        conf = pred['conf'].cpu().numpy().squeeze()  # Confidence map\n",
    "\n",
    "        # Determine the confidence threshold based on the percentile\n",
    "        conf_thr = np.percentile(conf, min_conf_thr_percentile)\n",
    "\n",
    "        # Apply confidence mask to filter points\n",
    "        mask = conf > conf_thr\n",
    "        world_points_filtered = world_points[mask]\n",
    "\n",
    "        # Generate 2D pixel coordinates corresponding to the filtered points\n",
    "        h, w, _ = world_points.shape\n",
    "        image_points = np.indices((h, w)).reshape(2, -1).T  # Shape: (N, 2)\n",
    "        image_points_filtered = image_points[mask.flatten()]  # Apply mask to 2D points\n",
    "\n",
    "        if world_points_filtered.shape[0] == 0:\n",
    "            print(f\"View {i}: No points above confidence threshold. Skipping camera estimation.\")\n",
    "            continue\n",
    "\n",
    "        # Estimate the camera matrix\n",
    "        P = estimate_camera_matrix(world_points_filtered, image_points_filtered)\n",
    "        print(f\"Camera matrix for view {i}:\\n\", P)\n",
    "\n",
    "        # Decompose into intrinsic and extrinsic matrices\n",
    "        K, R, t = decompose_camera_matrix(P)\n",
    "        print(f\"Intrinsic matrix (K) for view {i}:\\n\", K)\n",
    "        print(f\"Rotation matrix (R) for view {i}:\\n\", R)\n",
    "        print(f\"Translation vector (t) for view {i}:\\n\", t)\n",
    "\n",
    "        # Store the camera pose (rotation, translation, and intrinsic matrix)\n",
    "        camera_poses.append((R, t, K))\n",
    "    \n",
    "    return camera_poses\n",
    "\n",
    "# Estimate the camera poses first\n",
    "camera_poses = estimate_camera_poses(output, min_conf_thr_percentile=80)\n",
    "\n",
    "# Create a 3D plot and plot the 3D points together with the estimated cameras\n",
    "fig = go.Figure()\n",
    "plot_3d_points_with_estimated_camera(output, fig, camera_poses, min_conf_thr_percentile=50)\n",
    "\n",
    "# Display the final plot with 3D points and camera cones\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output['views'][0]['img'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Align with DTU point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Rt matrix of the first image lives at /fsx-cortex/jianingy/dust3r_data/datasets_raw/DTU/SampleSet/MVS Data/Calibration/cal18/pos_001.txt\n",
    "# it looks like this:\n",
    "# 2607.429996 -3.844898 1498.178098 -533936.661373\n",
    "# -192.076910 2862.552532 681.798177 23434.686572\n",
    "# -0.241605 -0.030951 0.969881 22.540121\n",
    "# I'd like to use this this to rotate an input 3D points to the correct orientation\n",
    "# my 3d points assumes the camera is at (0, 0, 0) and looking at (0, 0, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import trimesh\n",
    "import plotly.graph_objs as go\n",
    "from scipy.linalg import rq\n",
    "\n",
    "def load_camera_matrix(filepath):\n",
    "    \"\"\"Loads the camera calibration matrix from the given file.\"\"\"\n",
    "    with open(filepath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    camera_matrix = np.array([list(map(float, line.split())) for line in lines])\n",
    "    return camera_matrix\n",
    "\n",
    "def decompose_camera_matrix(camera_matrix):\n",
    "    \"\"\"Decomposes the camera calibration matrix into intrinsic matrix (K), rotation matrix (R), and translation vector (t).\"\"\"\n",
    "    # The camera matrix is 3x4\n",
    "    M = camera_matrix[:, :3]\n",
    "    \n",
    "    # RQ decomposition to separate K and R\n",
    "    K, R = rq(M)\n",
    "    \n",
    "    # Normalize K to ensure the sign of the diagonal is positive\n",
    "    T = np.diag(np.sign(np.diag(K)))\n",
    "    K = K @ T\n",
    "    R = T @ R\n",
    "    \n",
    "    # Compute translation vector t\n",
    "    t = np.linalg.inv(K) @ camera_matrix[:, 3]\n",
    "    \n",
    "    # Camera position C = -R^T * t\n",
    "    camera_position = -R.T @ t\n",
    "    \n",
    "    return K, R, t, camera_position\n",
    "\n",
    "def apply_transformation_to_point_cloud(ply_filepath, camera_matrix_filepath):\n",
    "    \"\"\"Applies the rotation and translation from the decomposed camera matrix to a point cloud loaded from a .ply file.\"\"\"\n",
    "    \n",
    "    # Load the point cloud\n",
    "    point_cloud = trimesh.load(ply_filepath)\n",
    "    \n",
    "    # Load and decompose the camera matrix\n",
    "    camera_matrix = load_camera_matrix(camera_matrix_filepath)\n",
    "    K, R, t, camera_position = decompose_camera_matrix(camera_matrix)\n",
    "\n",
    "    \n",
    "    # print point cloud range before transformation\n",
    "    print(f\"X range: {np.min(point_cloud.vertices[:, 0])} - {np.max(point_cloud.vertices[:, 0])} = {np.max(point_cloud.vertices[:, 0]) - np.min(point_cloud.vertices[:, 0])}\")\n",
    "    print(f\"Y range: {np.min(point_cloud.vertices[:, 1])} - {np.max(point_cloud.vertices[:, 1])} = {np.max(point_cloud.vertices[:, 1]) - np.min(point_cloud.vertices[:, 1])}\")\n",
    "    print(f\"Z range: {np.min(point_cloud.vertices[:, 2])} - {np.max(point_cloud.vertices[:, 2])} = {np.max(point_cloud.vertices[:, 2]) - np.min(point_cloud.vertices[:, 2])}\")\n",
    "\n",
    "    # prting the camera position\n",
    "    print(f\"Camera position: {camera_position}\")\n",
    "    \n",
    "    # Apply the rotation matrix to the point cloud vertices\n",
    "    rotated_points = (R @ point_cloud.vertices.T).T\n",
    "    \n",
    "    # Apply translation\n",
    "    transformed_points = rotated_points + t\n",
    "    \n",
    "    # Print the range of the transformed points per axis\n",
    "    print(f\"X range: {np.min(transformed_points[:, 0])} - {np.max(transformed_points[:, 0])} = {np.max(transformed_points[:, 0]) - np.min(transformed_points[:, 0])}\")\n",
    "    print(f\"Y range: {np.min(transformed_points[:, 1])} - {np.max(transformed_points[:, 1])} = {np.max(transformed_points[:, 1]) - np.min(transformed_points[:, 1])}\")\n",
    "    print(f\"Z range: {np.min(transformed_points[:, 2])} - {np.max(transformed_points[:, 2])} = {np.max(transformed_points[:, 2]) - np.min(transformed_points[:, 2])}\")\n",
    "    \n",
    "    # Create a new point cloud with rotated and translated points\n",
    "    transformed_point_cloud = trimesh.PointCloud(vertices=transformed_points, colors=point_cloud.colors)\n",
    "    \n",
    "    return transformed_point_cloud\n",
    "\n",
    "def plot_point_cloud(point_cloud, title=\"Transformed Point Cloud\"):\n",
    "    \"\"\"Visualizes a point cloud using Plotly.\"\"\"\n",
    "    x = point_cloud.vertices[:, 0]\n",
    "    y = point_cloud.vertices[:, 1]\n",
    "    z = point_cloud.vertices[:, 2]\n",
    "    colors = point_cloud.colors / 255.0  # Normalize colors to [0, 1] for Plotly\n",
    "    \n",
    "    fig = go.Figure(data=[go.Scatter3d(\n",
    "        x=x, y=y, z=z,\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=2,\n",
    "            color=colors,\n",
    "            opacity=0.8\n",
    "        )\n",
    "    )])\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        scene=dict(\n",
    "            xaxis_title='X',\n",
    "            yaxis_title='Y',\n",
    "            zaxis_title='Z'\n",
    "        ),\n",
    "        margin=dict(l=0, r=0, b=0, t=40),\n",
    "        height=800\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "# Example usage:\n",
    "ply_filepath = '/opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/jianingy/research/accel-cortex/dust3r/fast3r/notebooks/output/combined_mesh.ply'\n",
    "camera_matrix_filepath = '/fsx-cortex/jianingy/dust3r_data/datasets_raw/DTU/SampleSet/MVS Data/Calibration/cal18/pos_001.txt'\n",
    "\n",
    "transformed_point_cloud = apply_transformation_to_point_cloud(ply_filepath, camera_matrix_filepath)\n",
    "\n",
    "# Save the transformed point cloud to a new .ply file\n",
    "transformed_point_cloud.export('/opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/jianingy/research/accel-cortex/dust3r/fast3r/notebooks/output/transformed_output.ply')\n",
    "\n",
    "# Visualize the transformed point cloud\n",
    "plot_point_cloud(transformed_point_cloud)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import trimesh\n",
    "\n",
    "def load_and_print_xyz_ranges(ply_filepath):\n",
    "    \"\"\"Loads a point cloud from a .ply file and prints the XYZ ranges.\"\"\"\n",
    "    \n",
    "    # Load the point cloud\n",
    "    point_cloud = trimesh.load(ply_filepath)\n",
    "    \n",
    "    # Extract the vertices (XYZ coordinates)\n",
    "    vertices = point_cloud.vertices\n",
    "    \n",
    "    # Calculate the ranges for X, Y, and Z\n",
    "    x_min, x_max = np.min(vertices[:, 0]), np.max(vertices[:, 0])\n",
    "    y_min, y_max = np.min(vertices[:, 1]), np.max(vertices[:, 1])\n",
    "    z_min, z_max = np.min(vertices[:, 2]), np.max(vertices[:, 2])\n",
    "    \n",
    "    # Print the ranges\n",
    "    print(f\"X range: {x_min} - {x_max} = {x_max - x_min}\")\n",
    "    print(f\"Y range: {y_min} - {y_max} = {y_max - y_min}\")\n",
    "    print(f\"Z range: {z_min} - {z_max} = {z_max - z_min}\")\n",
    "\n",
    "# Example usage:\n",
    "reference_ply_filepath = '/fsx-cortex/jianingy/dust3r_data/datasets_raw/DTU/SampleSet/MVS Data/Points/stl/stl006_total.ply'\n",
    "\n",
    "load_and_print_xyz_ranges(reference_ply_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "import os\n",
    "from scipy.linalg import rq\n",
    "\n",
    "def load_camera_matrix(filepath):\n",
    "    \"\"\"Loads the camera calibration matrix from the given file.\"\"\"\n",
    "    with open(filepath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    camera_matrix = np.array([list(map(float, line.split())) for line in lines])\n",
    "    return camera_matrix\n",
    "\n",
    "def decompose_camera_matrix(camera_matrix):\n",
    "    \"\"\"Decomposes the camera calibration matrix into intrinsic matrix (K), rotation matrix (R), and translation vector (t).\"\"\"\n",
    "    # The camera matrix is 3x4\n",
    "    M = camera_matrix[:, :3]\n",
    "    \n",
    "    # RQ decomposition to separate K and R\n",
    "    K, R = rq(M)\n",
    "    \n",
    "    # Normalize K to ensure the sign of the diagonal is positive\n",
    "    T = np.diag(np.sign(np.diag(K)))\n",
    "    K = K @ T\n",
    "    R = T @ R\n",
    "    \n",
    "    # Compute translation vector t\n",
    "    t = np.linalg.inv(K) @ camera_matrix[:, 3]\n",
    "    \n",
    "    # Camera position C = -R^T * t\n",
    "    camera_position = -R.T @ t\n",
    "    \n",
    "    return K, R, t, camera_position\n",
    "\n",
    "def plot_camera_poses(base_path, pose_count):\n",
    "    \"\"\"Plots all camera poses and visualizes them in Plotly.\"\"\"\n",
    "    camera_positions = []\n",
    "    camera_orientations = []\n",
    "    \n",
    "    for i in range(1, pose_count + 1):\n",
    "        filepath = os.path.join(base_path, f'pos_{i:03d}.txt')\n",
    "        camera_matrix = load_camera_matrix(filepath)\n",
    "        \n",
    "        # Print the full camera matrix\n",
    "        print(f\"Camera Matrix {i}:\\n{camera_matrix}\\n\")\n",
    "        \n",
    "        K, R, t, camera_position = decompose_camera_matrix(camera_matrix)\n",
    "        \n",
    "        # Print the decomposed matrices\n",
    "        print(f\"Intrinsic Matrix (K) {i}:\\n{K}\\n\")\n",
    "        print(f\"Rotation Matrix (R) {i}:\\n{R}\\n\")\n",
    "        print(f\"Translation Vector (t) {i}:\\n{t}\\n\")\n",
    "        print(f\"Camera Position {i}: {camera_position}\\n\")\n",
    "        \n",
    "        # Camera direction (assuming camera is looking along -Z in its own coordinate system)\n",
    "        camera_direction = R.T @ np.array([0, 0, -1])\n",
    "        \n",
    "        camera_positions.append(camera_position)\n",
    "        camera_orientations.append(camera_direction)\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    camera_positions = np.array(camera_positions)\n",
    "    camera_orientations = np.array(camera_orientations)\n",
    "    \n",
    "    # Create the 3D scatter plot for camera positions\n",
    "    scatter = go.Scatter3d(\n",
    "        x=camera_positions[:, 0],\n",
    "        y=camera_positions[:, 1],\n",
    "        z=camera_positions[:, 2],\n",
    "        mode='markers',\n",
    "        marker=dict(size=5, color='blue'),\n",
    "        name='Camera Positions'\n",
    "    )\n",
    "    \n",
    "    # Create the 3D quiver plot for camera orientations\n",
    "    quiver = go.Cone(\n",
    "        x=camera_positions[:, 0],\n",
    "        y=camera_positions[:, 1],\n",
    "        z=camera_positions[:, 2],\n",
    "        u=camera_orientations[:, 0],\n",
    "        v=camera_orientations[:, 1],\n",
    "        w=camera_orientations[:, 2],\n",
    "        sizemode='scaled',\n",
    "        sizeref=2,\n",
    "        colorscale='Blues',\n",
    "        name='Camera Orientations'\n",
    "    )\n",
    "    \n",
    "    # Set up the layout\n",
    "    layout = go.Layout(\n",
    "        title='Camera Poses Visualization',\n",
    "        scene=dict(\n",
    "            xaxis=dict(title='X'),\n",
    "            yaxis=dict(title='Y'),\n",
    "            zaxis=dict(title='Z'),\n",
    "        ),\n",
    "        margin=dict(l=0, r=0, b=0, t=40),\n",
    "        height=800\n",
    "    )\n",
    "    \n",
    "    # Create the figure and show it\n",
    "    fig = go.Figure(data=[scatter, quiver], layout=layout)\n",
    "    fig.show()\n",
    "\n",
    "# Example usage:\n",
    "base_path = '/fsx-cortex/jianingy/dust3r_data/datasets_raw/DTU/SampleSet/MVS Data/Calibration/cal18'\n",
    "pose_count = 49  # Adjust this according to the number of poses available\n",
    "\n",
    "plot_camera_poses(base_path, pose_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dust3r",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
