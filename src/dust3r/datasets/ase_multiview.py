import os
import os.path as osp
import time
import cv2
import math
import numpy as np
import torch
import random
from torch.utils.data import Dataset
from tqdm import tqdm
from scipy.spatial.transform import Rotation as R
from PIL import Image

from src.dust3r.datasets.base.base_stereo_view_dataset import BaseStereoViewDataset
from src.dust3r.utils.image import imread_cv2
from src.dust3r.datasets.aria.camera_utils import undistort_fisheye_to_pinhole_rgbd, VignetteCorrector

##########################################
# Camera parameters and utilities
##########################################

FISHEYE_CAM_PARAMS = torch.tensor(
    [
        297.638,
        357.66,
        349.192,
        0.365089,
        -0.173808,
        -0.753495,
        2.43479,
        -2.57786,
        0.878848,
        0.00080052,
        -0.000294238,
        0,
        0,
        0,
        0,
    ],
    dtype=torch.float32,
)[None, None, :]

PINHOLE_CAM_PARAMS = torch.tensor(
    [297.638, 297.638, 357.66, 349.192], dtype=torch.float32
)[None, None, :]

ASE_INTRINSICS = torch.tensor(
    [
        [PINHOLE_CAM_PARAMS[0, 0, 0], 0, PINHOLE_CAM_PARAMS[0, 0, 2]],
        [0, PINHOLE_CAM_PARAMS[0, 0, 1], PINHOLE_CAM_PARAMS[0, 0, 3]],
        [0, 0, 1],
    ],
    dtype=torch.float32,
)

T_DEVICE_FROM_CAMERA = torch.tensor(
    [
        [0.99606003, -0.04388682, 0.07706079, -0.0075301],
        [0.08210934, 0.78468796, -0.61442889, -0.01090855],
        [-0.03350334, 0.61833547, 0.78519983, -0.00359806],
        [0.0, 0.0, 0.0, 1.0],
    ],
    dtype=torch.float32,
)

def imread_cv2(path, flags=cv2.IMREAD_COLOR):
    img = cv2.imread(path, flags)
    if img is None:
        raise FileNotFoundError(f"Image at {path} not found.")
    return img

def read_trajectory_file(filepath):
    assert osp.exists(filepath), f"Could not find trajectory file: {filepath}"
    with open(filepath, "r") as f:
        header = f.readline()
        positions = []
        rotations = []
        transforms = []
        timestamps = []
        for line in f.readlines():
            line = line.strip().split(",")
            timestamp = int(line[1])
            translation = np.array([float(line[3]), float(line[4]), float(line[5])], dtype=np.float32)
            quat_xyzw = np.array([float(line[6]), float(line[7]), float(line[8]), float(line[9])], dtype=np.float32)
            rot_matrix = R.from_quat(quat_xyzw).as_matrix()
            transform = np.eye(4, dtype=np.float32)
            transform[:3,:3] = rot_matrix
            transform[:3,3] = translation
            positions.append(translation)
            rotations.append(rot_matrix)
            transforms.append(transform)
            timestamps.append(timestamp)
    return {
        "positions": np.stack(positions),
        "rotations": np.stack(rotations),
        "Ts_world_from_device": np.stack(transforms),
        "timestamps": np.array(timestamps),
    }


##########################################
# Rotation about camera Z-axis
##########################################
def get_rotation_matrix_z(k):
    # k times 90 degrees clockwise about Z
    # For k=1 (90 deg cw): Rz = [[0,1,0],[-1,0,0],[0,0,1]]
    # For k=2 (180 deg): Rz = [[-1,0,0],[0,-1,0],[0,0,1]]
    # For k=3 (270 deg cw): Rz = [[0,-1,0],[1,0,0],[0,0,1]]
    k = k % 4
    if k == 0:
        return torch.eye(4, dtype=torch.float32)
    elif k == 1:
        R = torch.tensor([[0,1,0],
                          [-1,0,0],
                          [0,0,1]], dtype=torch.float32)
    elif k == 2:
        R = torch.tensor([[-1,0,0],
                          [0,-1,0],
                          [0,0,1]], dtype=torch.float32)
    elif k == 3:
        R = torch.tensor([[0,-1,0],
                          [1,0,0],
                          [0,0,1]], dtype=torch.float32)
    Rt = torch.eye(4, dtype=torch.float32)
    Rt[:3,:3] = R
    return Rt

def rotate_image_90_clockwise(img):
    return cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)

def adjust_intrinsics_for_90_clockwise_rotation(K, original_width, original_height):
    fx = K[0,0]
    fy = K[1,1]
    cx = K[0,2]
    cy = K[1,2]

    new_width = original_height
    new_height = original_width

    # After a 90Â° clockwise rotation:
    # fx' = fy
    # fy' = fx
    # cx' = cy
    # cy' = (W - 1) - cx
    new_fx = fy
    new_fy = fx
    new_cx = cy
    new_cy = (original_width - 1) - cx

    K_new = np.array([
        [new_fx, 0, new_cx],
        [0, new_fy, new_cy],
        [0,    0,   1   ]
    ], dtype=np.float32)
    return K_new

##########################################
# ASE_Multiview dataset
##########################################
class ASE_Multiview(BaseStereoViewDataset):
    def __init__(
        self,
        ROOT,
        split="train",
        num_views=4,
        window_size=10,
        num_samples_per_window=10,
        data_scaling=1.0,
        ordered=False,
        *args,
        **kwargs
    ):
        super().__init__(*args, split=split, **kwargs)
        self.ROOT = ROOT
        self.split = split
        self.num_views = num_views
        self.window_size = window_size
        self.num_samples_per_window = num_samples_per_window
        self.data_scaling = data_scaling
        self.ordered = ordered

        # Load scenes
        self.scenes = sorted([d for d in os.listdir(self.ROOT) if osp.isdir(osp.join(self.ROOT, d))])[:1]
        if self.data_scaling < 1.0:
            num_scenes = max(1, int(len(self.scenes)*self.data_scaling))
            self.scenes = self.scenes[:num_scenes]

        self.scene_to_indices = {}
        self.metadata = []
        # Load frames from each scene
        for scene_id, scene_name in enumerate(self.scenes):
            scene_path = osp.join(self.ROOT, scene_name)
            trajectory = read_trajectory_file(osp.join(scene_path, "trajectory.csv"))
            n_frames = len(trajectory["Ts_world_from_device"])
            for frame_idx in range(n_frames):
                self.metadata.append((scene_id, frame_idx, scene_name, trajectory))
        
        from collections import defaultdict
        group_by_scene = defaultdict(list)
        for i,(scene_id, frame_idx, scene_name, traj) in enumerate(self.metadata):
            group_by_scene[scene_id].append(i)

        # Sort by frame_idx within each scene
        for sid in group_by_scene:
            group_by_scene[sid].sort(key=lambda i: self.metadata[i][1])

        self.scene_to_indices = group_by_scene
        self._generate_combinations()

        self.vignette_corrector = VignetteCorrector()

    def _generate_combinations(self):
        self.combinations = []
        for scene_id, indices in self.scene_to_indices.items():
            if len(indices) < self.num_views:
                continue
            max_index_diff = self.window_size
            for i, idx_center in enumerate(indices):
                # form a window around idx_center
                window_start = max(0, i - max_index_diff // 2)
                window_end = min(len(indices), i + max_index_diff // 2)
                window_indices = indices[window_start:window_end]

                for _ in range(self.num_samples_per_window):
                    if len(window_indices) >= self.num_views:
                        combo = random.sample(window_indices, self.num_views)
                        if self.ordered:
                            combo = sorted(combo, key=lambda x: window_indices.index(x))
                        self.combinations.append(tuple(combo))

        self.combinations = sorted(set(self.combinations))

    def __len__(self):
        return len(self.combinations)

    def _get_views(self, idx, resolution, rng):
        start_time = time.time()
        image_indices = self.combinations[idx]
        views = []
        for view_idx in image_indices:
            scene_id, frame_idx, scene_name, trajectory = self.metadata[view_idx]
            scene_dir = osp.join(self.ROOT, scene_name)

            # Load pose
            pose = trajectory["Ts_world_from_device"][frame_idx].copy()
            # Apply device->camera transform
            pose = pose @ T_DEVICE_FROM_CAMERA.numpy()

            # Load image and depth
            rgb_path = osp.join(scene_dir, "rgb", f"vignette{frame_idx:07d}.jpg")
            depth_path = osp.join(scene_dir, "depth", f"depth{frame_idx:07d}.png")
            rgb = imread_cv2(rgb_path, cv2.IMREAD_COLOR).astype(np.float32)
            depth = imread_cv2(depth_path, cv2.IMREAD_UNCHANGED).astype(np.float32)

            # Vignette correct
            rgb = self.vignette_corrector.correct(rgb)

            # Undistort (fisheye to pinhole)
            rgb_undistorted, depth_undistorted = undistort_fisheye_to_pinhole_rgbd(
                rgb, depth, FISHEYE_CAM_PARAMS, PINHOLE_CAM_PARAMS
            )

            # Rotate image and depth 90 deg clockwise
            rgb_rotated = rotate_image_90_clockwise(rgb_undistorted)
            depth_rotated = rotate_image_90_clockwise(depth_undistorted)

            # Adjust intrinsics for rotation
            H, W, _ = rgb_undistorted.shape
            intrinsics = ASE_INTRINSICS.clone().numpy()
            intrinsics = adjust_intrinsics_for_90_clockwise_rotation(intrinsics, W, H)

            # Also rotate the pose about camera Z to match the image rotation
            # Rotation 90 deg clockwise about camera Z:
            Rz = get_rotation_matrix_z(k=1).numpy()
            pose = pose @ Rz

            # Convert depth to meters
            depthmap = depth_rotated / 1000.0

            # Convert image to uint8 and create PIL image
            rgb_rotated_uint8 = np.clip(rgb_rotated, 0, 255).astype(np.uint8)
            rgb_image = Image.fromarray(rgb_rotated_uint8, mode="RGB")

            rgb_image, depthmap, intrinsics = self._crop_resize_if_necessary(
                rgb_image, depthmap, intrinsics, resolution, rng=rng, info=view_idx)

            views.append(dict(
                img=rgb_image,
                depthmap=depthmap.astype(np.float32),
                camera_pose=pose.astype(np.float32),
                camera_intrinsics=intrinsics.astype(np.float32),
                dataset='ASE',
                label=scene_name + f"_{frame_idx:07d}",
                instance=f'{idx}_{view_idx}',
            ))

        print(f"Time taken for loading views: {time.time() - start_time:.2f}s")
        return views
